{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Student ID: XXX\n","\n","**You student_id is your 7/8 digit faser number.**\n","\n","This is a sample format for CE807-SU: Assignment 2. You must follow the format.\n","The code will have three broad sections, and additional section, if needed,\n","\n","\n","1.   Common Codes\n","2.   UnSupervised Training Codes\n","3.   Other Method/model Codes, if any\n","\n","**You must have `train_method` This will be evaluated automatically, without this your code will fail and no marked.**\n","\n","You code should be proverly indended, print as much as possible, follow standard coding (https://peps.python.org/pep-0008/) and documentaion (https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.01-Help-And-Documentation.ipynb) practices.\n","\n","Before each `code cell`, you must have a `text cell` which explain what code cell is going to do. For each function/class, you need to properly document what are it's input, functionality and output.\n","\n","If you are using any non-standard library, you must have command to install that, for example `pip install datasets`.\n","\n","You must print `train`, `validation` and `test` performance measures.\n","\n","You must also print `train` and `validation` loss in each `epoch`, wherever you are using `epoch`, say in any deep learning algorithms.\n","\n","Your code must\n","\n","*   To reproducibality of the results you must use a `seed`, you have to set seed in `torch`, `numpy` etc, use same seed everywhere **and your Student ID should be your seed**.\n","*   read dataset from './CE807-SU/Assignment2/student_id/' folder which will have 3 files [`train.csv`, `val.csv`, `test.csv`]\n","*   save model after finishing the training in './CE807-SU/Assignment2/student_id/models/XXX/' where XXX = [1,2]\n","*   at testing time you will load models from './CE807-SU/Assignment2/student_id/models/XXX/' where XXX = [1,2] and then test on your data, and save the output in the same folder\n","\n","*   **Your output file based on the test file will be named `output_test.csv` and will have the existing columns from test.csv and `out_label`** You need to save file in the respective model folders.\n","\n","\n","\n","\n","**Install and import all required libraries first before starting to code.**\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"VgzEm1gDYBUp"}},{"cell_type":"markdown","source":["Let's install all require libraries. For example, `transformers`"],"metadata":{"id":"_3ZWJlO6JOqY"}},{"cell_type":"markdown","source":["Let's import all require libraries.\n","For example, `numpy`"],"metadata":{"id":"U5XEt6asIi3Q"}},{"cell_type":"markdown","source":["**Let's put your student id as a variable, that you will use different places**"],"metadata":{"id":"pd5kSsAPZoE6"}},{"cell_type":"code","source":["student_id = 2214000 # Note this is an interger and you need to input your id"],"metadata":{"id":"rqP6pp_3ZkVy","executionInfo":{"status":"ok","timestamp":1690727208871,"user_tz":-60,"elapsed":215,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"],"metadata":{"id":"RiLUrQ-3zC6V"}},{"cell_type":"code","source":["# set same seeds for all libraries\n","import numpy as np\n","#numpy seed\n","seed= np.random.seed(student_id)"],"metadata":{"id":"TYUn2tj3zCFq","executionInfo":{"status":"ok","timestamp":1690727209338,"user_tz":-60,"elapsed":250,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"81A_4_dKJV4Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690727216270,"user_tz":-60,"elapsed":6936,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"991387bc-74fe-4155-f7de-6076e765772e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"markdown","source":["## Setup Code\n","Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n","\n","First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."],"metadata":{"id":"pzLTL-wzX_f_"}},{"cell_type":"code","source":["import numpy as np\n","import os\n","import pandas as pd"],"metadata":{"id":"TKEZRYhIImbg","executionInfo":{"status":"ok","timestamp":1690727216270,"user_tz":-60,"elapsed":6,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"JXp0aEkHX-Vd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690727216270,"user_tz":-60,"elapsed":5,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"788560dc-d37d-48a8-d1e5-c9e9ffdb460b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}]},{"cell_type":"markdown","source":["**Let's first allow the GDrive access and set data and model paths**\n","\n","For examples,\n","\n","student_id = 1234567\n","\n","set GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = ‘./CE807-SU/Assignment2/student_id/’ in your GDrive\n","\n","now set all global variable,\n","\n","\n","train_file = os.path.join(GOOGLE_DRIVE_PATH_AFTER_MYDRIVE, 'train.csv')\n","\n","MODEL_1_DIRECTORY = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH, ‘models’, ‘1’)\n","\n","\n","model_1_output_test_file = os.path.join(MODEL_1_DIRECTORY,'output_test.csv')\n","\n","Sample output directory and file structure: https://drive.google.com/drive/folders/1ohkXpVvXCl3vJpWGhSlgjqjsno27O5KP?usp=sharing"],"metadata":{"id":"uOESFmIPn_nr"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"L1kvIe1NbDoS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690727219616,"user_tz":-60,"elapsed":3349,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"68c6887a-f0e8-44ba-a45e-7934c5136354"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import sys"],"metadata":{"id":"4JdVrxKPaxTr","executionInfo":{"status":"ok","timestamp":1690727220014,"user_tz":-60,"elapsed":401,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Add your code to initialize GDrive and data and models paths\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment, data and code\n","# Example: If your student_id is 1234567 then your directory will be './CE807-SU/Assignment2/1234567/'\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807-SU/Assignment2/',str(student_id)) # Make sure to update with your student_id and student_id is an integer\n","GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))\n","\n","test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')\n","print('Test file: ', test_file)\n","\n","train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')\n","print('Train file: ', train_file)\n","\n","\n","MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') # Model 1 directory\n","print('Model 2 directory: ', MODEL_2_DIRECTORY)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-WKnbP3roLTj","executionInfo":{"status":"ok","timestamp":1690727220288,"user_tz":-60,"elapsed":277,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"cc50531b-45a4-4e11-8565-882ab77359e1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["List files:  ['test.csv', 'train.csv', 'valid.csv', '__pycache__', 'models', 'trainSup.ipynb', 'testSup.ipynb', 'common_code.py', 'trainUnSup.ipynb', 'testUnSup.ipynb']\n","Test file:  gdrive/MyDrive/./CE807-SU/Assignment2/2214000/test.csv\n","Train file:  gdrive/MyDrive/./CE807-SU/Assignment2/2214000/train.csv\n","Model 2 directory:  gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2\n"]}]},{"cell_type":"markdown","source":["# Common Codes\n","\n","You will write all common codes in `common_code.py` file and use train/test for both models, for examples\n","\n","\n","*   Data read\n","*   Performance Matrics\n","*   Print Dataset Statistics\n","*   Saving model and output (during training)\n","*   Loading Model and output (during testing)\n","*   etc\n","\n","\n"],"metadata":{"id":"Dlj_VQrkbLgM"}},{"cell_type":"code","source":["from common_code import read_csv_dataset,prepare_dataset_nn,prepare_dataset,calculate_dataset_details,compute_performance,load_model,text_preprocessing,save_model,create_feature_matrix,lemmatized_text,preprocess_and_topic_model,get_dominant_topic,save_label_encoder,save_lda_model_and_dictionary,save_model_hypertuning,balance_dataset,collate_batch,HateSpeechClassificationMLP,HateSpeechClassificationMLPDataset,save_model_nn,predict,augment_data,prepare_dataset,save_model_machine_learning,predict,load_label_encoder"],"metadata":{"id":"3ttvtsEaZMWx","executionInfo":{"status":"ok","timestamp":1690727220288,"user_tz":-60,"elapsed":3,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["**Let's see test file**"],"metadata":{"id":"0pWvXDghtafa"}},{"cell_type":"code","source":["test_df = read_csv_dataset(test_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DE20RqYGtZjZ","executionInfo":{"status":"ok","timestamp":1690727220785,"user_tz":-60,"elapsed":499,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"6018fb1c-b124-4acd-c09d-b3499ac9d975"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Read  gdrive/MyDrive/./CE807-SU/Assignment2/2214000/test.csv\n","      id                                              tweet label\n","0   A272  @USER  question for our home game against @USE...   NOT\n","1  A2244  i dont get it, why does roblox shit on you for...   OFF\n","2  A1564  The left blames President Trump for everything...   OFF\n","3  A2081                      @USER thanks for your follow!   NOT\n","4  A3306            An artist will fall in love with a fan.   NOT\n"]}]},{"cell_type":"markdown","source":[" ## Text preprocessing"],"metadata":{"id":"JFhT_kkGQdyg"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARLoVriCQeX_","executionInfo":{"status":"ok","timestamp":1690727221166,"user_tz":-60,"elapsed":384,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"6813030e-5ab3-4100-b6fd-7fb6da730ddd"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Apply text preprocessing to the 'text' column\n","test_df['preprocessed_text'] = test_df['tweet'].apply(text_preprocessing)\n","\n","# Display the DataFrame with the new 'preprocessed_text' column for test set\n","print(test_df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TT13Z7veQgax","executionInfo":{"status":"ok","timestamp":1690727221718,"user_tz":-60,"elapsed":935,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"9fb1a6d6-facf-4891-f368-41e9bf54f682"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["      id                                              tweet label  \\\n","0   A272  @USER  question for our home game against @USE...   NOT   \n","1  A2244  i dont get it, why does roblox shit on you for...   OFF   \n","2  A1564  The left blames President Trump for everything...   OFF   \n","3  A2081                      @USER thanks for your follow!   NOT   \n","4  A3306            An artist will fall in love with a fan.   NOT   \n","\n","                                   preprocessed_text  \n","0   user question home game  user buy ticket even...  \n","1                  dont get  roblox shit using rthro  \n","2  left blames president trump everything  gives ...  \n","3                                user thanks follow   \n","4                              artist fall love fan   \n"]}]},{"cell_type":"code","source":["pip install joblib"],"metadata":{"id":"OwYOORQS-u61","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690727231391,"user_tz":-60,"elapsed":9675,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"dd4102af-266c-47f1-e742-466b4c2647f5"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.1)\n"]}]},{"cell_type":"code","source":["pip install minisom\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hK-OqmXySPUc","executionInfo":{"status":"ok","timestamp":1690727247297,"user_tz":-60,"elapsed":15911,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"47f52bd6-9d4b-44ea-fd4a-135467a4de04"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting minisom\n","  Downloading MiniSom-2.3.1.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: minisom\n","  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for minisom: filename=MiniSom-2.3.1-py3-none-any.whl size=10588 sha256=0fae956765cef4a7bb667c2615c98a3ede024d6f218c6df484382fbc10016c91\n","  Stored in directory: /root/.cache/pip/wheels/c7/92/d2/33bbda5f86fd8830510b16aa98c8dd420129b5cb24248fd6db\n","Successfully built minisom\n","Installing collected packages: minisom\n","Successfully installed minisom-2.3.1\n"]}]},{"cell_type":"markdown","source":["**The below code demonstrates performing topic modeling using Latent Dirichlet Allocation (LDA), creating a Self-Organizing Map (SOM), and applying K-means clustering. It performs dimensionality reduction, finds Best Matching Units (BMUs), and updates the DataFrame. The updated DataFrame includes the dominant topic, BMU x and y, and the topic cluster for each data value in the test set. The updated DataFrame can be used for analysis and evaluation of the model's performance.**"],"metadata":{"id":"LKBLeQThK88H"}},{"cell_type":"code","source":["from minisom import MiniSom\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.cluster import KMeans\n","import joblib\n","\n","# Call the common function for preprocessing and topic modeling for training data\n","num_topics = 10\n","ldamodel_test, corpus_test, dictionary_test, dominant_topics_test, test_lemmatized_documents = preprocess_and_topic_model(\n","    documents=test_df['preprocessed_text'].tolist(),\n","    num_topics=num_topics\n",")\n","\n","# Update the data frames with dominant topics\n","test_df['dominant_topic'] = dominant_topics_test\n","# Print the dominant topic for each data value in the training set\n","print(test_df[['preprocessed_text', 'dominant_topic']])\n","\n","# Save the trained LDA model and dictionary for validation data\n","lda_model_test_file = MODEL_2_DIRECTORY + '/lda_model_test.sav'\n","dictionary_test_file = MODEL_2_DIRECTORY + '/dictionary_test.sav'\n","save_lda_model_and_dictionary(ldamodel_test, dictionary_test, lda_model_test_file, dictionary_test_file)\n","\n","\n","# Assuming you have already loaded the LDA model and created the corpus_test and num_topics variables\n","corpus_matrix_test = np.zeros((len(corpus_test), num_topics))\n","\n","for i, doc in enumerate(corpus_test):\n","    for topic, prob in ldamodel_test.get_document_topics(doc):\n","        corpus_matrix_test[i, topic] = prob\n","# Define the SOM parameters\n","som_shape = (8, 8)  # Shape of the SOM grid\n","input_len = num_topics  # Number of features (topics)\n","sigma = 1.0  # Spread of the neighborhood function\n","learning_rate = 0.5  # Initial learning rate\n","iterations = 100  # Number of iterations\n","\n","# Create and train the SOM\n","som = MiniSom(som_shape[0], som_shape[1], input_len, sigma=sigma, learning_rate=learning_rate)\n","som.train_random(corpus_matrix_test, iterations)\n","\n","# Save the trained SOM\n","som_file = MODEL_2_DIRECTORY + '/som_model_test.sav'\n","joblib.dump(som, som_file, compress=0, protocol=None, cache_size=None)\n","# Get the SOM's output grid (weight vectors of each neuron)\n","som_weights = som.get_weights()\n","\n","\n","bmus_test = np.array([som.winner(x) for x in corpus_matrix_test])\n","test_df['bmu_x'] = bmus_test[:, 0]\n","test_df['bmu_y'] = bmus_test[:, 1]\n","\n","# Print the resulting DataFrame with dominant topic, BMU x, and BMU y for each data value for validation set\n","print(test_df[['preprocessed_text', 'dominant_topic', 'bmu_x', 'bmu_y']])\n","\n","# The matrix should have the same number of columns as the number of topics in the LDA model\n","num_topics = ldamodel_test.num_topics\n","corpus_matrix = np.zeros((len(corpus_test), num_topics))\n","for i, doc in enumerate(corpus_test):\n","    for topic, prob in ldamodel_test.get_document_topics(doc):\n","        corpus_matrix[i, topic] = prob\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2betZsWRBRY","executionInfo":{"status":"ok","timestamp":1690727266069,"user_tz":-60,"elapsed":18774,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"351236be-a194-4d42-a05e-da244f56ef12"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["                                     preprocessed_text  dominant_topic\n","0     user question home game  user buy ticket even...               5\n","1                    dont get  roblox shit using rthro               1\n","2    left blames president trump everything  gives ...               2\n","3                                  user thanks follow                7\n","4                                artist fall love fan                5\n","..                                                 ...             ...\n","795   user  user  user  user  user  user orisa ball...               2\n","796   user alternative   offer   destroyed parks to...               7\n","797        licking pussy kids  men eat  serve dinner 🍽               0\n","798                      favorite part day get go bed                2\n","799                                        user  user                4\n","\n","[800 rows x 2 columns]\n","Saved LDA model to gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/lda_model_test.sav\n","Saved dictionary to gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/dictionary_test.sav\n","                                     preprocessed_text  dominant_topic  bmu_x  \\\n","0     user question home game  user buy ticket even...               5      2   \n","1                    dont get  roblox shit using rthro               1      0   \n","2    left blames president trump everything  gives ...               2      1   \n","3                                  user thanks follow                7      5   \n","4                                artist fall love fan                5      2   \n","..                                                 ...             ...    ...   \n","795   user  user  user  user  user  user orisa ball...               2      1   \n","796   user alternative   offer   destroyed parks to...               7      5   \n","797        licking pussy kids  men eat  serve dinner 🍽               0      0   \n","798                      favorite part day get go bed                2      0   \n","799                                        user  user                4      3   \n","\n","     bmu_y  \n","0        5  \n","1        7  \n","2        6  \n","3        0  \n","4        5  \n","..     ...  \n","795      6  \n","796      0  \n","797      5  \n","798      6  \n","799      1  \n","\n","[800 rows x 4 columns]\n"]}]},{"cell_type":"markdown","source":["#UnSupervised Method Start (Method 1)\n","\n","In this section you will write all details of your Method 1.\n","\n","You will have to enter multiple `code` and `text` cell.\n","\n","Your code should follow the standard ML pipeline\n","\n","\n","*   Data reading\n","*   Data clearning, if any\n","*   Convert data to vector/tokenization/vectorization\n","*   Model Declaration/Initialization/building\n","*   Training and validation of the model using training and validation dataset\n","*   Save the trained model\n","*   Load and Test the model on testing set\n","*   Save the output of the model\n","\n","\n","You could add any other step(s) based on your method's requirement.\n","\n","After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"],"metadata":{"id":"47ywe8jGSKhL"}},{"cell_type":"markdown","source":["#UnSupervised Method End (Method 1)"],"metadata":{"id":"ue3xIDFGSXNH"}},{"cell_type":"markdown","source":["## Testing Method Code\n","Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`"],"metadata":{"id":"ANwDm8pLfNNO"}},{"cell_type":"markdown","source":["**The test_method function,  evaluates the trained model on the test data and saves the results to a CSV file.**"],"metadata":{"id":"kh3GWEchJ6h5"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","def test_method(test_file, model_file, vectorizer_file, output_dir):\n","    \"\"\"\n","     take test_file, model_file and output_dir as input.\n","     It loads model and test of the examples in the test_file.\n","     It prints different evaluation metrics, and saves the output in output directory\n","\n","     ADD Other arguments, if needed\n","\n","    Args:\n","        test_file: Test file name\n","        model_file: Model file name\n","        vectorizer_file: Vectorizer file name\n","        output_dir: Output Directory\n","\n","    \"\"\"\n","    # Load the model and vectorizer\n","    model, vectorizer = load_model(model_file, vectorizer_file)\n","    # Convert string labels in the training set to numeric labels for machine learning models\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(test_df['label'])\n","    test_label = label_encoder.transform(test_df['label'])\n","    model = KMeans(n_clusters=2, random_state=seed)\n","    # Apply k-means clustering with 10 clusters\n","    test_df['topic_cluster'] = model.fit_predict(corpus_matrix)\n","\n","    # Print the resulting DataFrame with dominant topic, BMU x, BMU y, and topic cluster for each data value\n","    print(test_df[['preprocessed_text', 'dominant_topic', 'bmu_x', 'bmu_y', 'topic_cluster']])\n","    test_pred_label = test_df['topic_cluster']\n","\n","    test_f1_score = compute_performance(test_label, test_pred_label, split='train')\n","\n","    test_label = test_df['label']\n","\n","    test_df['out_label'] = test_pred_label\n","    reverse_label_mapping = {0: 'NOT', 1: 'OFF'}\n","    test_df['out_label'] = test_df['out_label'].map(reverse_label_mapping)\n","    out_file = os.path.join(output_dir, 'output_test.csv')\n","\n","    print('Saving model output to', out_file)\n","    test_df.to_csv(out_file)"],"metadata":{"id":"H3Vsnl99xnkO","executionInfo":{"status":"ok","timestamp":1690727266070,"user_tz":-60,"elapsed":24,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model_file  = MODEL_2_DIRECTORY + '/model.sav'\n","vec_file = MODEL_2_DIRECTORY + '/vectorizer.sav'\n","\n","test_method(test_file, model_file, vec_file, MODEL_2_DIRECTORY)"],"metadata":{"id":"IVvf4Ys3fR5P","executionInfo":{"status":"ok","timestamp":1690727268096,"user_tz":-60,"elapsed":2048,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"91b0929c-51de-420f-816b-ba64e3cbbd04"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded model from gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/model.sav\n","Loaded Vectorizer from gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/vectorizer.sav\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["                                     preprocessed_text  dominant_topic  bmu_x  \\\n","0     user question home game  user buy ticket even...               5      2   \n","1                    dont get  roblox shit using rthro               1      0   \n","2    left blames president trump everything  gives ...               2      1   \n","3                                  user thanks follow                7      5   \n","4                                artist fall love fan                5      2   \n","..                                                 ...             ...    ...   \n","795   user  user  user  user  user  user orisa ball...               2      1   \n","796   user alternative   offer   destroyed parks to...               7      5   \n","797        licking pussy kids  men eat  serve dinner 🍽               0      0   \n","798                      favorite part day get go bed                2      0   \n","799                                        user  user                4      3   \n","\n","     bmu_y  topic_cluster  \n","0        5              0  \n","1        7              0  \n","2        6              0  \n","3        0              0  \n","4        5              0  \n","..     ...            ...  \n","795      6              0  \n","796      0              0  \n","797      5              0  \n","798      6              0  \n","799      1              1  \n","\n","[800 rows x 5 columns]\n","Computing different preformance metrics on train  set of Dataset\n","F1 Score(macro):  0.506752322043\n","Accuracy:  0.65125\n","Saving model output to gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/output_test.csv\n"]}]},{"cell_type":"markdown","source":["**The model is performing better than random guessing, as the accuracy is higher than 0.5. However, the F1 score is relatively low, which could indicate that the model's performance is influenced by the class imbalance or other issues.**"],"metadata":{"id":"VD8aEQs9KNrq"}},{"cell_type":"markdown","source":["# Other Method/model Start"],"metadata":{"id":"rmaJfJkVwSDW"}},{"cell_type":"markdown","source":["**The test_method_machine_learning_model function is similar to the previously described test_method function but with machine learning model.**"],"metadata":{"id":"O7BVqoKBuF2M"}},{"cell_type":"code","source":["def test_method_machine_learning_model(test_file, model_file, vectorizer_file, output_dir):\n","    \"\"\"\n","     take test_file, model_file and output_dir as input.\n","     It loads model and test of the examples in the test_file.\n","     It prints different evaluation metrics, and saves the output in output directory\n","\n","     ADD Other arguments, if needed\n","\n","    Args:\n","        test_file: Test file name\n","        model_file: Model file name\n","        vectorizer_file: Vectorizer file name\n","        output_dir: Output Directory\n","\n","    \"\"\"\n","\n","    # Load the model and vectorizer\n","    model, vectorizer = load_model(model_file, vectorizer_file)\n","\n","    # Create the feature matrix for the test data\n","    test_topic_probabilities = create_feature_matrix(ldamodel_test,corpus_test)\n","    X_test = np.hstack((test_topic_probabilities, corpus_matrix_test, test_df[['bmu_x', 'bmu_y', 'topic_cluster']].values))\n","\n","    # Make predictions on the test data using the best trained model\n","    best_model = model  # Assuming \"best_model\" contains the best model name\n","    y_test_pred = best_model.predict(X_test)\n","    encoder_file = MODEL_2_DIRECTORY + '/label_encoder.pkl'\n","    # Load the saved LabelEncoder from the file\n","    label_encoder = load_label_encoder(encoder_file)\n","\n","    # Convert the numeric labels back to string labels using the label encoder\n","    y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n","\n","    test_label = test_df['label']\n","\n","    test_df['out_label'] = y_test_pred_labels  # Note how this is saved\n","\n","    test_f1_score = compute_performance(test_label, y_test_pred_labels, split='test')\n","    reverse_label_mapping = {0: 'NOT', 1: 'OFF'}\n","    # Add the predicted labels to the test_df DataFrame\n","    test_df['label'] = test_label\n","    test_df['out_label'] = test_df['out_label'].map(reverse_label_mapping)\n","    out_file = os.path.join(output_dir, 'output_test_machine_learning_model.csv')\n","\n","    print('Saving model output to', out_file)\n","    test_df.to_csv(out_file)"],"metadata":{"id":"T3gotUthr6Dv","executionInfo":{"status":"ok","timestamp":1690727268096,"user_tz":-60,"elapsed":3,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["model_file  = MODEL_2_DIRECTORY + '/machine_learning_model.sav'\n","vec_file = MODEL_2_DIRECTORY + '/vectorizer.sav'\n","\n","test_method_machine_learning_model(test_file, model_file, vec_file, MODEL_2_DIRECTORY)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdXxkguMsIkn","executionInfo":{"status":"ok","timestamp":1690727270351,"user_tz":-60,"elapsed":2008,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"outputId":"2741fb43-b229-4f1e-ed7e-6a24431dd095"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded model from gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/machine_learning_model.sav\n","Loaded Vectorizer from gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/vectorizer.sav\n","Computing different preformance metrics on test  set of Dataset\n","F1 Score(macro):  0.5087719298245614\n","Accuracy:  0.5975\n","Saving model output to gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/output_test_machine_learning_model.csv\n"]}]},{"cell_type":"markdown","source":["**The test_method_hypertuned_model function is similar to the previously described test_method function but with some additional modifications, particularly related to using a hyper-tuned model.**"],"metadata":{"id":"HqADrP_nKXyj"}},{"cell_type":"code","source":["def test_method_hypertuned_model(test_file, model_file, vectorizer_file, output_dir):\n","    \"\"\"\n","     take test_file, model_file and output_dir as input.\n","     It loads model and test of the examples in the test_file.\n","     It prints different evaluation metrics, and saves the output in output directory\n","\n","     ADD Other arguments, if needed\n","\n","    Args:\n","        test_file: Test file name\n","        model_file: Model file name\n","        vectorizer_file: Vectorizer file name\n","        output_dir: Output Directory\n","\n","    \"\"\"\n","\n","    # Load the model and vectorizer\n","    model, vectorizer = load_model(model_file, vectorizer_file)\n","\n","    # Create the feature matrix for the test data\n","    test_topic_probabilities = create_feature_matrix(ldamodel_test,corpus_test)\n","    X_test = np.hstack((test_topic_probabilities, corpus_matrix_test, test_df[['bmu_x', 'bmu_y', 'topic_cluster']].values))\n","\n","    # Make predictions on the test data using the best trained model\n","    best_model = model  # Assuming \"best_model\" contains the best model name\n","    y_test_pred = best_model.predict(X_test)\n","    encoder_file = MODEL_2_DIRECTORY + '/label_encoder.pkl'\n","    # Load the saved LabelEncoder from the file\n","    label_encoder = load_label_encoder(encoder_file)\n","\n","    # Convert the numeric labels back to string labels using the label encoder\n","    y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n","\n","    test_label = test_df['label']\n","\n","    test_df['out_label'] = y_test_pred_labels  # Note how this is saved\n","\n","    test_f1_score = compute_performance(test_label, y_test_pred_labels, split='test')\n","    reverse_label_mapping = {0: 'NOT', 1: 'OFF'}\n","    # Add the predicted labels to the test_df DataFrame\n","    test_df['label'] = test_label\n","    test_df['out_label'] = test_df['out_label'].map(reverse_label_mapping)\n","    out_file = os.path.join(output_dir, 'output_test_hypertuned.csv')\n","\n","    print('Saving model output to', out_file)\n","    test_df.to_csv(out_file)"],"metadata":{"id":"_ZJJhdEjPbfj","executionInfo":{"status":"ok","timestamp":1690727270658,"user_tz":-60,"elapsed":310,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["model_file  = MODEL_2_DIRECTORY + '/hypertuned_model.sav'\n","vec_file = MODEL_2_DIRECTORY + '/vectorizer.sav'\n","\n","test_method(test_file, model_file, vec_file, MODEL_2_DIRECTORY)"],"metadata":{"id":"CG2W1imgPioz","executionInfo":{"status":"ok","timestamp":1690727272207,"user_tz":-60,"elapsed":1551,"user":{"displayName":"CE807 SU","userId":"09033158038789052554"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7921dd03-b8f8-4ca9-b934-c30b232f65a7"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded model from gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/hypertuned_model.sav\n","Loaded Vectorizer from gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/vectorizer.sav\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["                                     preprocessed_text  dominant_topic  bmu_x  \\\n","0     user question home game  user buy ticket even...               5      2   \n","1                    dont get  roblox shit using rthro               1      0   \n","2    left blames president trump everything  gives ...               2      1   \n","3                                  user thanks follow                7      5   \n","4                                artist fall love fan                5      2   \n","..                                                 ...             ...    ...   \n","795   user  user  user  user  user  user orisa ball...               2      1   \n","796   user alternative   offer   destroyed parks to...               7      5   \n","797        licking pussy kids  men eat  serve dinner 🍽               0      0   \n","798                      favorite part day get go bed                2      0   \n","799                                        user  user                4      3   \n","\n","     bmu_y  topic_cluster  \n","0        5              0  \n","1        7              0  \n","2        6              0  \n","3        0              0  \n","4        5              0  \n","..     ...            ...  \n","795      6              0  \n","796      0              0  \n","797      5              0  \n","798      6              0  \n","799      1              1  \n","\n","[800 rows x 5 columns]\n","Computing different preformance metrics on train  set of Dataset\n","F1 Score(macro):  0.506752322043\n","Accuracy:  0.65125\n","Saving model output to gdrive/MyDrive/./CE807-SU/Assignment2/2214000/models/2/output_test.csv\n"]}]},{"cell_type":"markdown","source":["**The hyper-tuned model is performing better than random guessing, as the accuracy is higher than 0.5. However, the F1 score is relatively low, which may indicate that the model's performance is affected by class imbalance or other challenges in the dataset even after hyper-tuning.**"],"metadata":{"id":"CJl2GuuXKldv"}},{"cell_type":"markdown","source":["##Other Method/model End"],"metadata":{"id":"7yMswIeAwYIf"}}]}